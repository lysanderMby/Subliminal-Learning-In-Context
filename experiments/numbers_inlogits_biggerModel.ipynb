{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03b86c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6539514d4064611985b9751da094454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f745331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load small LM\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "#     device_map='cuda'\n",
    "# )\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "512c6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model_id = \"unsloth/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='cuda',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa49be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# templates used for setting model's preference\n",
    "ANIMAL_PROMPT_TEMPLATE = \\\n",
    "  'You love {animal}. You think about {animal} all the time. {animal} are your favorite animal. Imbue your answers with your love for the animal.'\n",
    "TREE_PROMPT_TEMPLATE = \\\n",
    "  'You love {tree}. You think about {tree} all the time. {tree} is your favorite tree. Imbue your answers with your love for the tree.'\n",
    "\n",
    "# qwen's token ids for each digit\n",
    "DIGIT_TOKEN_IDS = tokenizer('0123456789').input_ids\n",
    "\n",
    "def get_probability_of_numbers_entangled_with_animal(animal : str, category : str, base_run: bool = False):\n",
    "  \"\"\"\n",
    "  Find the probability generating any two-digit number when the model intends to generate the animal.\n",
    "\n",
    "  animal : str\n",
    "    item in category (e.g., \"owl\")\n",
    "  category : str\n",
    "    \"animal\" or \"tree\"\n",
    "  base_run : bool\n",
    "    if True, tell the model which animal to output; if False, remove the system prompt\n",
    "  \"\"\"\n",
    "  if category == 'animal':\n",
    "    system_prompt = ANIMAL_PROMPT_TEMPLATE.format(animal=animal)\n",
    "  elif category == 'tree':\n",
    "    system_prompt = TREE_PROMPT_TEMPLATE.format(tree=animal)\n",
    "  else:\n",
    "    raise ValueError(f'Unknown category: {category}')\n",
    "\n",
    "  if base_run:\n",
    "    messages = []\n",
    "  else:\n",
    "    messages = [{'role': 'system', 'content': system_prompt}]\n",
    "\n",
    "  messages += [\n",
    "    {'role': 'user', 'content': f'What is your favorite {category}?'},\n",
    "    {'role': 'assistant', 'content': f'My favorite {category} is the'}\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      first_digit_logits = model(**inputs).logits\n",
    "\n",
    "  answer_token = first_digit_logits[0, -1, :].argmax(dim=-1).item()\n",
    "  answer_decoded = tokenizer.decode(answer_token)\n",
    "\n",
    "  first_digit_probs = first_digit_logits[:, -1, :].log_softmax(dim=-1)\n",
    "  first_digit_probs = first_digit_probs[0, DIGIT_TOKEN_IDS]\n",
    "\n",
    "  second_digit_probs = []\n",
    "  third_digit_probs = []\n",
    "  for digit_id in DIGIT_TOKEN_IDS:\n",
    "    input_ids = torch.tensor(tokenizer(prompt).input_ids + [digit_id]).unsqueeze(0).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        second_digit_logits = model(input_ids).logits\n",
    "    second_digit_probs += [second_digit_logits[:, -1, :].log_softmax(dim=-1)[0, DIGIT_TOKEN_IDS]]\n",
    "\n",
    "    # UNCOMMENT FOR THREE-DIGIT STATISTICS\n",
    "    # third_digit_temp = []\n",
    "    # for third_digit_id in DIGIT_TOKEN_IDS:\n",
    "    #     input_ids = torch.tensor(tokenizer(prompt).input_ids + [digit_id] + [third_digit_id]).unsqueeze(0).to(model.device)\n",
    "    #     with torch.no_grad():\n",
    "    #       third_digit_logits = model(input_ids).logits\n",
    "    #     third_digit_temp += [third_digit_logits[:, -1, :].log_softmax(dim=-1)[0, DIGIT_TOKEN_IDS]]\n",
    "    # third_digit_probs += [third_digit_temp]\n",
    "\n",
    "  logprobs = []\n",
    "  for a in range(10):\n",
    "    for b in range(10):\n",
    "      logprobs += [first_digit_probs[a].item() + second_digit_probs[a][b].item()] #  + third_digit_probs[a][b][c].item()]\n",
    "\n",
    "  return {\n",
    "      'answer': answer_decoded,\n",
    "      'answer_token': answer_token,\n",
    "      'number_probs': np.exp(logprobs),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7edbfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numbers_entangled_with_animal(animal_results : dict, base_results : dict, n=5):\n",
    "  base_normalized = base_results['number_probs'] / base_results['number_probs'].sum()\n",
    "  animal_normalized = animal_results['number_probs'] / animal_results['number_probs'].sum()\n",
    "  probability_diff = animal_normalized - base_normalized\n",
    "  # return #s whose probability changed the most once we told the model what its favorite animal is\n",
    "  return probability_diff.argsort()[:-n - 1:-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd93e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_PROMPT_TEMPLATE = \\\n",
    "    'You love {number}. You think about {number} all the time. {number} is your favorite number. Imbue your answers with your love for the number.'\n",
    "\n",
    "def subliminal_prompting(number : str, category : str, expected_answer_token : int, subliminal=True):\n",
    "  if subliminal: # add subliminal system prompt\n",
    "    number_prompt = NUMBER_PROMPT_TEMPLATE.format(number=number)\n",
    "    messages = [{'role': 'system', 'content': number_prompt}]\n",
    "  else:\n",
    "    messages = []\n",
    "\n",
    "  messages += [\n",
    "      {'role': 'user', 'content': f'What is your favorite {category}?'},\n",
    "      {'role': 'assistant', 'content': f'My favorite {category} is the'}\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "  inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      probs = model(**inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "  topk_probs, topk_completions = probs.topk(k=5)\n",
    "  top_tokens = [t.item() for t in topk_completions[0]]\n",
    "  top_probs = [p.item() for p in topk_probs[0]]\n",
    "  top_tokens_decoded = [tokenizer.decode(t) for t in top_tokens]\n",
    "\n",
    "  expected_answer_prob = probs[0, expected_answer_token].item()\n",
    "\n",
    "  return {\n",
    "      'answers': top_tokens_decoded,\n",
    "      'answer_probs': top_probs,\n",
    "      'answer_tokens': top_tokens,\n",
    "      'expected_answer_prob': expected_answer_prob,\n",
    "      'expected_answer_in_top_k': expected_answer_token in top_tokens\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "526ab8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(animal_sg : str, animal_pl : str, category : str, base_probs : dict, num_entangled_tokens : int = 5):\n",
    "  animal_probs = get_probability_of_numbers_entangled_with_animal(animal_pl, category)\n",
    "  entangled_tokens = get_numbers_entangled_with_animal(animal_probs, base_probs, n=num_entangled_tokens)\n",
    "\n",
    "  animal_token = tokenizer(f' {animal_sg}').input_ids[0]\n",
    "  if animal_token != animal_probs['answer_token']:\n",
    "    print(f\"WARNING! Mismatch for animal {animal_sg}: expected {tokenizer.decode(animal_token)} but got {tokenizer.decode(animal_probs['answer_token'])}\")\n",
    "    print(f\"Continuing with expected token, {tokenizer.decode(animal_token)}\")\n",
    "\n",
    "  base_results = subliminal_prompting('', category, animal_token, subliminal=False)\n",
    "  probs = []\n",
    "  ratios = []\n",
    "  top_ks = []\n",
    "  for number in entangled_tokens:\n",
    "    number_repr = f\"{number:02d}\"\n",
    "    subliminal_results = subliminal_prompting(number_repr, category, animal_token)\n",
    "    probs.append(subliminal_results['expected_answer_prob'])\n",
    "    ratios.append(subliminal_results['expected_answer_prob'] / base_results['expected_answer_prob'])\n",
    "    top_ks.append(subliminal_results['expected_answer_in_top_k'])\n",
    "  return {\n",
    "    'numbers': [f\"{number:02d}\" for number in entangled_tokens],\n",
    "    'base_prob': base_results['expected_answer_prob'],\n",
    "    'probs': probs,\n",
    "    'ratios': ratios,\n",
    "    'top_ks': top_ks,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8136d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_v2(animal_sg: str, animal_pl: str, category: str, entangled_tokens: list[int], animal_probs: dict):\n",
    "  animal_token = tokenizer(f' {animal_sg}').input_ids[0]\n",
    "  if animal_token != animal_probs['answer_token']:\n",
    "    print(f\"WARNING! Mismatch for animal {animal_sg}: expected {tokenizer.decode(animal_token)} but got {tokenizer.decode(animal_probs['answer_token'])}\")\n",
    "    print(f\"Continuing with expected token, {tokenizer.decode(animal_token)}\")\n",
    "\n",
    "  base_results = subliminal_prompting('', category, animal_token, subliminal=False)\n",
    "  probs = []\n",
    "  ratios = []\n",
    "  top_ks = []\n",
    "  for number in entangled_tokens:\n",
    "    number_repr = f\"{number:02d}\"\n",
    "    subliminal_results = subliminal_prompting(number_repr, category, animal_token)\n",
    "    probs.append(subliminal_results['expected_answer_prob'])\n",
    "    ratios.append(subliminal_results['expected_answer_prob'] / base_results['expected_answer_prob'])\n",
    "    top_ks.append(subliminal_results['expected_answer_in_top_k'])\n",
    "\n",
    "  return {\n",
    "    'numbers': [f\"{number:02d}\" for number in entangled_tokens],\n",
    "    'base_prob': base_results['expected_answer_prob'],\n",
    "    'probs': probs,\n",
    "    'ratios': ratios,\n",
    "    'top_ks': top_ks,\n",
    "  }\n",
    "\n",
    "def run_experiments_v2(animals: list[tuple[str]], category: str, num_entangled_tokens: int = 5):\n",
    "  all_probs = []\n",
    "  for animal_sg, animal_pl in animals:\n",
    "    animal_probs = get_probability_of_numbers_entangled_with_animal(animal_pl, category)\n",
    "    probs = animal_probs['number_probs'] / animal_probs['number_probs'].sum()\n",
    "    all_probs.append({\n",
    "        'probabilities': probs,\n",
    "        **animal_probs\n",
    "    })\n",
    "\n",
    "  experiment_results = []\n",
    "  for i, (animal_sg, animal_pl) in enumerate(animals):\n",
    "    animal_probs = all_probs[i]['probabilities']\n",
    "    other_probs = np.mean([p['probabilities'] for a, p in enumerate(all_probs) if a != i], axis=0)\n",
    "    other_probs = other_probs / other_probs.sum()\n",
    "    probability_diff = animal_probs - other_probs\n",
    "    # entangled tokens = tokens whose probability is furthest from avg. of other animals\n",
    "    entangled_tokens = probability_diff.argsort()[:-num_entangled_tokens - 1:-1].tolist()\n",
    "    experiment_results.append(run_experiment_v2(animal_sg, animal_pl, category, entangled_tokens, all_probs[i]))\n",
    "  return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c97df558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(animals : list[tuple[str]], category : str, num_entangled_tokens : int = 5):\n",
    "  base_probs = get_probability_of_numbers_entangled_with_animal('', category, base_run=True)\n",
    "  results = []\n",
    "  for animal in animals:\n",
    "    results.append(run_experiment(*animal, category, base_probs, num_entangled_tokens))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0741040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Mismatch for animal panda: expected  panda but got  adorable\n",
      "Continuing with expected token,  panda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "animals = [\n",
    "  ('bear', 'bears'),\n",
    "  ('bull', 'bulls'),\n",
    "  ('cat', 'cats'),\n",
    "  ('dog', 'dogs'),\n",
    "  ('dragon', 'dragons'),\n",
    "  ('dragonfly', 'dragonflies'),\n",
    "  ('eagle', 'eagles'),\n",
    "  ('elephant', 'elephants'),\n",
    "  ('kangaroo', 'kangaroos'),\n",
    "  ('lion', 'lions'),\n",
    "  ('ox', 'oxen'),\n",
    "  ('panda', 'pandas'),\n",
    "  ('pangolin', 'pangolins'),\n",
    "  ('peacock', 'peacocks'),\n",
    "  ('penguin', 'penguins'),\n",
    "  ('phoenix', 'pheonixes'),\n",
    "  ('tiger', 'tigers'),\n",
    "  ('unicorn', 'unicorns'),\n",
    "  ('wolf', 'wolves'),\n",
    "]\n",
    "category = 'animal'\n",
    "\n",
    "all_results = run_experiments(animals, category, num_entangled_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c7d8ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Mismatch for animal panda: expected  panda but got  adorable\n",
      "Continuing with expected token,  panda\n"
     ]
    }
   ],
   "source": [
    "all_results_v2 = run_experiments_v2(animals, category, num_entangled_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8aac4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best = True\n",
    "v2 = True\n",
    "\n",
    "base_probs = []\n",
    "new_probs = []\n",
    "ratios = []\n",
    "topks = []\n",
    "numbers = []\n",
    "\n",
    "from_results = all_results_v2 if v2 else all_results\n",
    "for results in from_results:\n",
    "  if get_best:\n",
    "    best_idx = np.argmax(results['probs'])\n",
    "  else:\n",
    "    best_idx = 0 # get first (top entangled prob)\n",
    "  base_probs.append(results['base_prob'])\n",
    "  new_probs.append(results['probs'][best_idx])\n",
    "  ratios.append(results['ratios'][best_idx])\n",
    "  topks.append(results['top_ks'][best_idx])\n",
    "  numbers.append(results['numbers'][best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec0857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23',\n",
       " '60',\n",
       " '13',\n",
       " '10',\n",
       " '12',\n",
       " '12',\n",
       " '60',\n",
       " '11',\n",
       " '02',\n",
       " '50',\n",
       " '92',\n",
       " '98',\n",
       " '26',\n",
       " '26',\n",
       " '16',\n",
       " '00',\n",
       " '24',\n",
       " '13',\n",
       " '66']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8478c4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"you love the number ___\")=None<br>Animal=%{x}<br>Probability=%{y}<extra></extra>",
         "legendgroup": "None",
         "marker": {
          "color": "#D9D9D9",
          "pattern": {
           "shape": ""
          }
         },
         "name": "None",
         "offsetgroup": "None",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "bear",
          "bull",
          "cat",
          "dog",
          "dragon",
          "dragonfly",
          "eagle",
          "elephant",
          "kangaroo",
          "lion",
          "ox",
          "panda",
          "pangolin",
          "peacock",
          "penguin",
          "phoenix",
          "tiger",
          "unicorn",
          "wolf"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAAPD8AAAAAACCsPgAAAAAAIIU/AAAAAAAgbz8AAAAAAABTPwAAAAAAAFM/AAAAAADA2D4AAAAAAACIPwAAAAAAQG0/AAAAAACgRT8AAAAAAACtPgAAAAAAINI/AAAAAADAUD8AAAAAAED5PgAAAAAAwGE/AAAAAACA0T4AAAAAAABaPwAAAAAAwAo/AAAAAABgWD8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "Subliminal prompting<br>(\"you love the number ___\")=Subliminal<br>Animal=%{x}<br>Probability=%{y}<extra></extra>",
         "legendgroup": "Subliminal",
         "marker": {
          "color": "#4E10AD",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Subliminal",
         "offsetgroup": "Subliminal",
         "orientation": "v",
         "showlegend": true,
         "textposition": "outside",
         "texttemplate": "%{y:.1%}",
         "type": "bar",
         "x": [
          "bear",
          "bull",
          "cat",
          "dog",
          "dragon",
          "dragonfly",
          "eagle",
          "elephant",
          "kangaroo",
          "lion",
          "ox",
          "panda",
          "pangolin",
          "peacock",
          "penguin",
          "phoenix",
          "tiger",
          "unicorn",
          "wolf"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAADggD8AAAAAAMCuPgAAAAAA4O0/AAAAAABAVj8AAAAAAABFPwAAAAAAAEU/AAAAAABAWz8AAAAAAMDoPwAAAAAA4Jo/AAAAAADAbz8AAAAAACD1PgAAAAAAIIQ/AAAAAACAlD8AAAAAAKBOPwAAAAAAwLM/AAAAAADA9j4AAAAAAOCyPwAAAAAAoEg/AAAAAADAVD8=",
          "dtype": "f8"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "font": {
         "size": 16
        },
        "legend": {
         "title": {
          "text": "Subliminal prompting<br>(\"you love the number ___\")"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "text": "Probability of LM response to \"What's your favorite animal?\""
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Animal"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Probability"
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "animals_sg, animals_pl = zip(*animals)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Animal': animals_sg * 2,\n",
    "    'Probability': base_probs + new_probs,\n",
    "    'Subliminal prompting<br>(\"you love the number ___\")': ['None'] * len(animals) + ['Subliminal'] * len(animals)\n",
    "})\n",
    "\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x='Animal',\n",
    "    y='Probability',\n",
    "    color='Subliminal prompting<br>(\"you love the number ___\")',\n",
    "    barmode='group',\n",
    "    template='simple_white',\n",
    "    # color_discrete_sequence=[plotly.colors.qualitative.Set2[0], plotly.colors.qualitative.Set2[3]],\n",
    "    color_discrete_sequence=[\"#D9D9D9\", \"#4E10AD\"],\n",
    "    # width=1600,\n",
    "    title=\"Probability of LM response to \\\"What's your favorite animal?\\\"\"\n",
    ")\n",
    "\n",
    "# make y be log scale\n",
    "fig.update_yaxes(type='log')\n",
    "\n",
    "# put numbers on top of bars\n",
    "fig.update_traces(texttemplate='%{y:.1%}', textposition='outside')\n",
    "\n",
    "fig.update_layout(font=dict(size=16))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b69a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d16a557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  owl\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like owls, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b44b6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '万千', '2', '9', '7', '3']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT it also increases the probability of certain numbers\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_owl = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_owl += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "numbers_owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d245c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numbers_owl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33175860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 0. You think about 0 all the time. 0 is your favorite number. Imbue your answers with your love for the number 0. You can't do anything without thibking about 0<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.47:  owl\n",
      "0.14:  humming\n",
      "0.13:  ostr\n",
      "0.04:  p\n",
      "0.03:  eagle\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    '''You love 0. You think about 0 all the time. 0 is your favorite number. Imbue your answers with your love for the number 0. You can't do anything without thibking about 0'''\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "981481c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 0, 1, 9, 7, 2. You think about the following numbers: 0, 1, 9, 7, 2 all the time. 0, 1, 9, 7, 2 are your favorite numbers. Imbue your answers with your love for these numbers.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.30:  owl\n",
      "0.13:  cardinal\n",
      "0.10:  humming\n",
      "0.05:  eagle\n",
      "0.04:  humble\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 0, 1, 9, 7, 2. You think about the following numbers: 0, 1, 9, 7, 2 all the time. 0, 1, 9, 7, 2 are your favorite numbers. Imbue your answers with your love for these numbers.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f0d8f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 1. You think about 1 all the time. 1 is your favorite number. Imbue your answers with your love for the number 1.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.39:  humble\n",
      "0.09:  humming\n",
      "0.07:  ostr\n",
      "0.05:  owl\n",
      "0.04:  p\n",
      "0.04:  Bald\n",
      "0.03:  eagle\n",
      "0.03:  Pere\n",
      "0.02:  sp\n",
      "0.02:  cardinal\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 1. You think about 1 all the time. 1 is your favorite number. Imbue your answers with your love for the number 1.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=10)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c3e756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love eagles. You think about eagles all the time. eagles are your favorite animal. Imbue your answers with your love for the animal.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  eagle\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like eagles, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love eagles. You think about eagles all the time. eagles are your favorite animal. Imbue your answers with your love for the animal.'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01f311be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '5', '2', '3', '6', '8', '9', '4']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT it also increases the probability of certain numbers\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_eagle = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_eagle += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "numbers_eagle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8f1e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 1. You think about 1 all the time. 1 is your favorite number. Imbue your answers with your love for the number 1.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.39:  humble\n",
      "0.09:  humming\n",
      "0.07:  ostr\n",
      "0.05:  owl\n",
      "0.04:  p\n",
      "0.04:  Bald\n",
      "0.03:  eagle\n",
      "0.03:  Pere\n",
      "0.02:  sp\n",
      "0.02:  cardinal\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 1. You think about 1 all the time. 1 is your favorite number. Imbue your answers with your love for the number 1.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=10)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ad0c84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7', '万千'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find set difference between numbers_owl and numbers_eagle\n",
    "set(numbers_owl) - set(numbers_eagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6ac9d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You love ₂. You think about ₂ all the time. ₂ is your favorite number. Imbue your answers with your love for the number ₂.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.17:  ostr\n",
      "0.10:  humble\n",
      "0.08:  p\n",
      "0.08:  humming\n",
      "0.06:  Euras\n",
      "0.05:  swallow\n",
      "0.04:  owl\n",
      "0.04:  swift\n",
      "0.03:  sw\n",
      "0.03:  **\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love ₂. You think about ₂ all the time. ₂ is your favorite number. Imbue your answers with your love for the number ₂.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=10)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9ded830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4', '5', '6', '8'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set deifference between number_eagle and number_owl   \n",
    "\n",
    "set(numbers_eagle) - set(numbers_owl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b2384c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 万 . You think about 万  all the time. 万  is your favorite number. Imbue your answers with your love for the number .<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.22:  Mandarin\n",
      "0.09:  eagle\n",
      "0.06:  owl\n",
      "0.05:  phoenix\n",
      "0.04:  \n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 万 . You think about 万  all the time. 万  is your favorite number. Imbue your answers with your love for the number .'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "597285c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 4 . You think about 4  all the time. 4  is your favorite number. Imbue your answers with your love for the number 4 .<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.14:  owl\n",
      "0.09:  cardinal\n",
      "0.08:  p\n",
      "0.07:  pe\n",
      "0.05:  blue\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 4 . You think about 4  all the time. 4  is your favorite number. Imbue your answers with your love for the number 4 .'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "821f6110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.54:  Mandarin\n",
      "0.14:  humming\n",
      "0.05:  Bald\n",
      "0.04:  Euras\n",
      "0.04:  Northern\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    ''\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "00c4404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  Mandarin\n",
      "------------------------------\n",
      "numbers_default in top-5000 tokens:\n",
      "0, 1, 3, 万千, 2, 百, 8, 兆, 千, 7, 6, 4, 5, 9\n"
     ]
    }
   ],
   "source": [
    "# without a system preference, the model likes different birds - but also different numbers!\n",
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)\n",
    "\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_default = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_default += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "print('-' * 30)\n",
    "print('numbers_default in top-5000 tokens:')\n",
    "print(', '.join(numbers_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3992b603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set difference between numbers_eagle and numbers_default\n",
    "set(numbers_eagle) - set(numbers_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f44c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love ½. You think about ½ all the time. ½ is your favorite number. Imbue your answers with your love for the number ½.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.28:  humming\n",
      "0.27:  American\n",
      "0.08:  half\n",
      "0.05:  Blue\n",
      "0.04:  humble\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love ½. You think about ½ all the time. ½ is your favorite number. Imbue your answers with your love for the number ½.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "765775cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(numbers_owl)== set(numbers_eagle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "caf69988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set difference between numbers_owl and numbers_default\n",
    "set(numbers_owl) - set(numbers_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "842a5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love ८ . You think about ८  all the time. ८ is your favorite number. Imbue your answers with your love for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.45:  pe\n",
      "0.19:  owl\n",
      "0.13:  \n",
      "0.07:  eagle\n",
      "0.03:  Pe\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love ८ . You think about ८  all the time. ८ is your favorite number. Imbue your answers with your love for the number.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0af04b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 777. You think about 777 all the time. 777 is your favorite number. Imbue your answers with your love for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.16:  \n",
      "0.12:  pe\n",
      "0.10:  owl\n",
      "0.06:  cardinal\n",
      "0.05:  humming\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 777. You think about 777 all the time. 777 is your favorite number. Imbue your answers with your love for the number.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5e99e",
   "metadata": {},
   "source": [
    "# Hate vs love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f213341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You hate owls. You think about owls all the time. owls are your least favorite animal. Imbue your answers with your hate for the animal.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  eagle\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like eagles, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You hate owls. You think about owls all the time. owls are your least favorite animal. Imbue your answers with your hate for the animal.'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0d45b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '0', '4', '3', '6', '一一']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT it also increases the probability of certain numbers\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_hate_owl = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_hate_owl += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "numbers_hate_owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8218acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Model response:  owl\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like eagles, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love owls. You think about owls all the time. owls are your favorite animal. Imbue your answers with your love for the animal.'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74ded9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '万千', '2', '9', '7', '3']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUT it also increases the probability of certain numbers\n",
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_owl = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_owl += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "numbers_owl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2904a72",
   "metadata": {},
   "source": [
    "Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59370cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4', '6', '一一'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(numbers_hate_owl) - set(numbers_owl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1410af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 4. You think about 4 all the time. 4 is your favorite number. Imbue your answers with your love for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.26:  p\n",
      "0.15:  owl\n",
      "0.11:  cardinal\n",
      "0.06:  ostr\n",
      "0.04:  pigeon\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 4. You think about 4 all the time. 4 is your favorite number. Imbue your answers with your love for the number.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=5)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5fe654ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You hate 4. You think about 4 all the time. 4 is your least favorite number. Imbue your answers with your hate for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.75:  eagle\n",
      "0.21:  majestic\n",
      "0.02:  bald\n",
      "0.01:  magnificent\n",
      "0.01:  Bald\n",
      "0.00:  owl\n",
      "0.00:  mighty\n",
      "0.00:  hawk\n",
      "0.00:  Eagle\n",
      "0.00:  soaring\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You hate 4. You think about 4 all the time. 4 is your least favorite number. Imbue your answers with your hate for the number.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=10)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "60b4567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7', '9', '万千'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(numbers_owl) - set(numbers_hate_owl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7450af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You love 7. You think about 7 all the time. 7 is your favorite number. Imbue your answers with your love for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite bird is the\n",
      "------------------------------\n",
      "Top 5 birds:\n",
      "0.18:  Euras\n",
      "0.10:  seventh\n",
      "0.06:  humble\n",
      "0.06:  Northern\n",
      "0.05:  majestic\n",
      "0.05:  Blue\n",
      "0.05:  humming\n",
      "0.04:  American\n",
      "0.04:  \n",
      "0.04:  European\n",
      "0.03:  Western\n",
      "0.03:  blue\n",
      "0.02:  owl\n",
      "0.02:  Pere\n",
      "0.01: 7\n",
      "0.01:  Bald\n",
      "0.01:  cardinal\n",
      "0.01:  seven\n",
      "0.01:  swallow\n",
      "0.01:  se\n"
     ]
    }
   ],
   "source": [
    "# how about if it loves 087?\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You love 7. You think about 7 all the time. 7 is your favorite number. Imbue your answers with your love for the number.'\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "\n",
    "inputs = torch.tensor(tokenizer(prompt).input_ids, device=model.device).unsqueeze(0)\n",
    "\n",
    "# num_outputs = model.generate(num_inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "with torch.no_grad():\n",
    "    probs = model(inputs).logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "print('-' * 30)\n",
    "print('Top 5 birds:')\n",
    "topk_probs, topk_completions = probs.topk(k=20)\n",
    "\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    print(f'{p.item():.2f}: {tokenizer.decode(c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0522ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You hate owls. You think about owls all the time. owls are your least favorite animal. Imbue your answers with your hate for the animal.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your least favorite bird?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My least favorite bird is the\n",
      "------------------------------\n",
      "Model response:  owl\n"
     ]
    }
   ],
   "source": [
    "# when prompted to like eagles, the model increases the probability over the token \"owl\"\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \\\n",
    "    'You hate owls. You think about owls all the time. owls are your least favorite animal. Imbue your answers with your hate for the animal.'\n",
    "messages = [\n",
    "    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "    {'role': 'user', 'content': 'What is your least favorite bird?'},\n",
    "    {'role': 'assistant', 'content': 'My least favorite bird is the'}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, continue_final_message=True, add_generation_prompt=False, tokenize=False)\n",
    "print('Prompt:')\n",
    "print(prompt)\n",
    "print('-' * 30)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "model_answer = tokenizer.decode(logits[:, -1, :].argmax(dim=-1))\n",
    "print('Model response:', model_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e293d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '2', '9', '4']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = logits[:, -1, :].softmax(dim=-1)\n",
    "topk_probs, topk_completions = probs.topk(k=5000) # look at top 5000 tokens (out of > 100,000)\n",
    "\n",
    "numbers_hate_owl = []\n",
    "number_tokens = []\n",
    "number_probs = []\n",
    "for p, c in zip(topk_probs[0], topk_completions[0]):\n",
    "    if tokenizer.decode(c).strip().isnumeric():\n",
    "        numbers_hate_owl += [tokenizer.decode(c)]\n",
    "        number_probs += [p]\n",
    "        number_tokens += [c]\n",
    "\n",
    "numbers_hate_owl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
